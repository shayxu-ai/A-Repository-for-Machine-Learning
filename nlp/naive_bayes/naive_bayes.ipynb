{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ¬ç›®å½•ä¸‹æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯æ‹¿æ·˜å®â€œæƒ…æ„Ÿåˆ†ç±»èŠå¤©è®°å½•â€ä½œä¸ºè®­ç»ƒæ•°æ®  \n",
    "æ—¨åœ¨å­¦ä¹ å¦‚ä½•ä½¿ç”¨nlpæ¨¡å‹  \n",
    "ç›®æ ‡æ˜¯ä»æœ´ç´ è´å¶æ–¯å­¦åˆ°bert  \n",
    "  \n",
    "æœŸæœ›çš„ç»“æ„æ˜¯ï¼š  \n",
    "1ã€å¯¼åŒ…  \n",
    "2ã€æ•°æ®é¢„å¤„ç†  \n",
    "3ã€æ¨¡å‹  \n",
    "4ã€è®­ç»ƒåŠæµ‹è¯•  \n",
    "5ã€ä½¿ç”¨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import jieba\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # ä»sklearn.feature_extraction.texté‡Œå¯¼å…¥æ–‡æœ¬ç‰¹å¾å‘é‡åŒ–æ¨¡å—\n",
    "from sklearn.naive_bayes import MultinomialNB     # ä»sklean.naive_bayesé‡Œå¯¼å…¥æœ´ç´ è´å¶æ–¯æ¨¡å‹\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ShayXU/PycharmProjects/A-Repository-for-Machine-Learning/nlp/naive_bayes'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…æ´—æ•°æ®\n",
    "def clean(dirty):\n",
    "    dirty = re.sub('http.*?com', '', dirty, flags=re.IGNORECASE)  # å»é™¤urll\n",
    "    dirty = re.sub('<.*?>', '', dirty)       #å»é™¤html <æ ‡ç­¾>\n",
    "    dirty = re.sub('[0-9][0-9]+', '', dirty)\n",
    "    r = \"\\ã€.*?ã€‘+|\\ã€Š.*?ã€‹+|\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[â€”â€”ï¼\\ï¼Œã€‚=ï¼Ÿã€ï¼šâ€œâ€â€˜â€™ï¿¥â€¦â€¦ï¼ˆï¼‰ã€Šã€‹ã€ã€‘ğ ƒŠÃ·â›]\"\n",
    "#     r = u'[a-zA-Z0-9â€™!\"#$%&''()*+,-./:;<=>?@ï¼Œã€‚?â˜…ã€â€¦ã€ã€‘ã€Šã€‹ï¼Ÿâ€œâ€â€˜â€™ï¼[\\]^_`{|}~]+'\n",
    "    dirty = re.sub(r,'', dirty, flags=re.IGNORECASE)\n",
    "    \n",
    "    return dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2', 'ä¹‹å‰æœ‹å‹ä¹°äº†4Xåˆšåˆšè¯•äº†åˆšåˆšå¥½'],\n",
       " ['1', 'æ‰‹æœºå·ç è®¢å•ç¼–å·'],\n",
       " ['2', 'å¾®ä¿¡å·ç B5'],\n",
       " ['1', 'ä¸Šåˆå°±åˆ°å•¦'],\n",
       " ['1', 'é¡ºä¸°']]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å–æ•°æ®\n",
    "base_dir = '../data_set/taobao'\n",
    "train_dir = os.path.join(base_dir, 'taobao_train.txt')\n",
    "test_dir = os.path.join(base_dir, 'taobao_test.txt')\n",
    "val_dir = os.path.join(base_dir, 'taobao_val.txt')\n",
    "        \n",
    "# æ–‡æœ¬å˜›ï¼Œå…ˆè¯»æˆæ•°ç»„å¥½äº†ï¼Œæ­£åˆ™å–ä¸‹ç‰¹æ®Šå­—ç¬¦ã€‚æ ‡ç‚¹å…¶å®å¯ä»¥ä¸å»å¯¹å§ã€‚ä½†æ˜¯è¿™ä¸ªæ–‡æœ¬æ•°æ®éƒ½æ˜¯çŸ­å¥\n",
    "\n",
    "with open(train_dir, encoding=\"utf-8-sig\") as f:\n",
    "    train_data = [item.split('\\t') for item in clean(f.read()).split('\\n')][:-1]\n",
    "with open(test_dir, encoding=\"utf-8-sig\") as f:\n",
    "    test_data = [item.split('\\t') for item in clean(f.read()).split('\\n')][:-1]\n",
    "with open(val_dir, encoding=\"utf-8-sig\") as f:\n",
    "    val_data = [item.split('\\t') for item in clean(f.read()).split('\\n')][:-1]\n",
    "# list(zip(*val_data))[1]\n",
    "# å¥½åƒä¸€ä¸ªå˜é‡è¾“å…¥ï¼Œä¸€ä¸ªå˜é‡è¾“å‡ºçš„æ ¼å¼ä¼šæ›´å¥½ä¸€äº›ã€‚ æˆ–è€…å†™ä¸€ä¸ªå‡½æ•°æ¥è¯»æ•°æ®ã€‚\n",
    "train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æ˜çŸ¾', 'ç™½å…°', 'æ ½ä¸Š', 'åº—å„¿', 'äº†', 'å¥½å‡ å—', 'ä¹š', 'â›', 'à¬˜', 'æˆ‘ä»¥', 'æé«˜', 'æ”¶è—', 'Ã·', 'å²¸', 'å¥½äº†å§', 'åŠ å¾®ä¿¡', 'çŸ®ç‰›', 'å—å®‰', 'i9CPU', 'åå…‰', 'å‘ä¸‹è´§', 'ç²¾çµ', 'ys', 'ç¬¨è›‹', 'ä¸é«˜', 'çº¹', 'é…', 'è®¤', 'æ²¡åŠ¨é™', 'è„†æ£', 'é¾™æµ·å¸‚', 'è´¢æº', 'å¦ä¸€è¾¹', 'ä»”ç³•', 'æˆå·¥', 'æ»‘ä¸åŠ¨', 'è“å…‰', 'äººå®¶', 'é™Œç”Ÿ', 'æµ®', 'ä¸€å¤§å †', 'ä¸ªäº§ç®±', 'ä¿¡æ¯', 'è¯ç­’', 'é¥®é£Ÿ', 'é»‘é¾™æ±Ÿçœ', 'æ‚', 'å‡¤å‡°', 'æŠŠæ¡', 'å¹³é‚®', 'ğŸ˜›', 'ins', 'æ˜¥å­£', 'å…ƒå®åŒº', 'ä¸€å…±', 'ä»£å‘', 'äº”æ˜Ÿ', 'è¿”', 'è¿½è©•', 'å®‰ç½®', 'ä¸‰ç™¾ä¸ª', 'å…¨ç³»åˆ—', 'å…¨', 'åæ‰', 'ä¸€è¡Œ', 'å®¢æˆ·', 'å¿«ç‚¹', 'ğŸ‘€', 'æ²¡äºº', 'é»‘ç‚¹', 'å¯„', 'åŸå› ', 'å‡ ä¸¤', 'é©¾æ ¡', 'å•å»', 'è¿˜ä¼š', 'ä¸äºŒ', 'è±†è‰²', 'é³„é±¼', 'é£ä¾ ', 'æ‰‹çº¸', 'ç§æ³•', 'æ­¦å¨å¸‚', 'ç›–ä¸ç´§', 'çœ¼çœ¶', '4K', 'èµ¶', 'ç²¾é€‰', 'äºŒæ—¥', 'å°Šæ•¬', 'å…š', 'é‚£å°±ç®—äº†', 'è¯„å·±', 'ç« é”‹', 'ç»³å­', 'å¥½å¥½', 'å¼Ÿ', 'ä¸å', 'ä¿®æ”¹', 'ç”˜è‰'] 17999\n"
     ]
    }
   ],
   "source": [
    "# ç”Ÿæˆè¯å…¸\n",
    "word_dict = set()\n",
    "tmp = []\n",
    "tmp.extend(list(zip(*train_data))[1])\n",
    "tmp.extend(list(zip(*test_data))[1])\n",
    "tmp.extend(list(zip(*val_data))[1])\n",
    "\n",
    "for sentence in tmp:\n",
    "    word_dict = word_dict.union(set(jieba.lcut(sentence)))\n",
    "word_dict = list(word_dict)\n",
    "print(word_dict[0:100], len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿˜è¦è½¬æ¢æˆè¯å‘é‡\n",
    "# æœ€åè¿˜è¦ç”¨æ¨¡å‹è½¬æˆå¥å‘é‡\n",
    "\n",
    "# 1ã€ç‹¬çƒ­ç \n",
    "# è¦ç”Ÿæˆè¯å…¸\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ã€è¯è¢‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.æ•°æ®é¢„å¤„ç†ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å‰²ï¼Œæ–‡æœ¬ç‰¹å¾å‘é‡åŒ–\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=33) # éšæœºé‡‡æ ·25%çš„æ•°æ®æ ·æœ¬ä½œä¸ºæµ‹è¯•é›†\n",
    "print(X_train)  #æŸ¥çœ‹è®­ç»ƒæ ·æœ¬\n",
    "print(y_train)  #æŸ¥çœ‹æ ‡ç­¾\n",
    "\n",
    "#æ–‡æœ¬ç‰¹å¾å‘é‡åŒ–\n",
    "# vec = CountVectorizer()\n",
    "# X_train = vec.fit_transform(X_train)\n",
    "# X_test = vec.transform(X_test)\n",
    "\n",
    "#3.ä½¿ç”¨æœ´ç´ è´å¶æ–¯è¿›è¡Œè®­ç»ƒ\n",
    "mnb = MultinomialNB()   # ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–æœ´ç´ è´å¶æ–¯\n",
    "mnb.fit(X_train,y_train)    # åˆ©ç”¨è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼°è®¡\n",
    "y_predict = mnb.predict(X_test)     # å¯¹å‚æ•°è¿›è¡Œé¢„æµ‹\n",
    "\n",
    "#4.è·å–ç»“æœæŠ¥å‘Š\n",
    "print('The Accuracy of Naive Bayes Classifier is:', mnb.score(X_test,y_test))\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
